{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wTAMaso58Ts"
      },
      "source": [
        "# LLMs in production - Trace, Compile, Evals - by Weights & Biases\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altryne/llm-evals-workshop/blob/main/eval.ipynb) [![Weights & Biases](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.me/weave-workshop-jan)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Intro\n",
        "This notebook is accompanying a workshop, that will walk you through common patterns in building evaluations for LLMs, and useful rules of thumb to follow when doing so using [W&B Weave](https://wandb.me/weave-workshop-jan)\n",
        "\n",
        "We'll explore the following methodology for productizing robust LLM applications:\n",
        "\n",
        "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
        "\n",
        "\n",
        "Make sure to set your WANDB_API_KEY (get your key from [here](https://wandb.ai/authorize)) and OPENROUTER_API_KEY (or OPENAI_API_KEY if you have that) in the environment variables.\n",
        "\n",
        "If you're running in Colab, set the variables in the keys section on the left.\n",
        "\n",
        "If you want to self explore, find the `#TODO:` comments and replace them with your own code, then run the cell.\n",
        "\n",
        "Prepared by [Alex Volkov](https://twitter.com/altryne)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Lg4dwX58Tu",
        "outputId": "c7af2e9f-fefa-4e87-a271-e37ccfa9af57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llm-evals-workshop\n",
            "‚è≥ Installing packages\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Packages installed\n"
          ]
        }
      ],
      "source": [
        "# Install and read in required packages\n",
        "try:\n",
        "    import google.colab\n",
        "    !git clone -q --branch main https://github.com/altryne/llm-evals-workshop\n",
        "    %cd llm-evals-workshop\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "print('‚è≥ Installing packages')\n",
        "%pip install -q uv\n",
        "!uv pip install -q --system 'weave[scorers]' gradio set-env-colab-kaggle-dotenv tqdm ipywidgets requests openai pillow\n",
        "print('‚úÖ Packages installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "D3rhACOn58Tu",
        "outputId": "c228ee03-d599-481c-9d7b-42ac00a5cb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-22 18:36:38.720\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mset_env.set_env\u001b[0m:\u001b[36mset_env\u001b[0m:\u001b[36m110\u001b[0m - \u001b[33m\u001b[1m\n",
            "        Unable to set WANDB_API_KEY=WANDB_API_KEY,\n",
            "        not in colab or Secrets not set, not kaggle\n",
            "        or Secrets not set, no .env/dotenv/env file\n",
            "        in the current working dir or parent dirs.\u001b[0m\n",
            "\u001b[32m2025-02-22 18:36:39.102\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mset_env.set_env\u001b[0m:\u001b[36mset_env\u001b[0m:\u001b[36m110\u001b[0m - \u001b[33m\u001b[1m\n",
            "        Unable to set OPENAI_API_KEY=OPENAI_API_KEY,\n",
            "        not in colab or Secrets not set, not kaggle\n",
            "        or Secrets not set, no .env/dotenv/env file\n",
            "        in the current working dir or parent dirs.\u001b[0m\n",
            "\u001b[32m2025-02-22 18:36:39.357\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mset_env.set_env\u001b[0m:\u001b[36mset_env\u001b[0m:\u001b[36m110\u001b[0m - \u001b[33m\u001b[1m\n",
            "        Unable to set OPENROUTER_API_KEY=OPENROUTER_API_KEY,\n",
            "        not in colab or Secrets not set, not kaggle\n",
            "        or Secrets not set, no .env/dotenv/env file\n",
            "        in the current working dir or parent dirs.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please login to Weights & Biases (https://wandb.ai/) to continue:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzkoticha\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as Weights & Biases user: zkoticha.\n",
            "View Weave data at https://wandb.ai/zkoticha/aie-llm-evals-workshop/weave\n",
            "üì¶ Published to https://wandb.ai/zkoticha/aie-llm-evals-workshop/weave/objects/doomer_or_boomer/versions/MfppDkza1qvK772eNZWIU1XwwZbtwGQ8UQWWEcyZlfc\n",
            "üì¶ Published to https://wandb.ai/zkoticha/aie-llm-evals-workshop/weave/objects/reason/versions/Z3Do6YnUa9YHEuELfGyZtJt7JTkgb30oVBv04U4HWyc\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%load_ext gradio\n",
        "\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "from set_env import set_env\n",
        "import json\n",
        "from jinja2 import Environment, FileSystemLoader\n",
        "from datetime import datetime\n",
        "import random\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import weave\n",
        "from weave.flow.annotation_spec import AnnotationSpec\n",
        "\n",
        "load_dotenv()\n",
        "set_env(\"WANDB_API_KEY\")\n",
        "set_env(\"OPENAI_API_KEY\")\n",
        "set_env(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# initialize weave\n",
        "weave_api = weave.init('aie-llm-evals-workshop')\n",
        "\n",
        "# initialize annotations for this project\n",
        "annotation = weave.publish(AnnotationSpec(\n",
        "    name=\"Doomer or Boomer\",\n",
        "    description=\"Doomer or Boomer or Neither\",\n",
        "    field_schema={ \"type\": \"string\", \"enum\": [\"Doomer\", \"Boomer\", \"Neither\"],},\n",
        "), \"doomer_or_boomer\")\n",
        "\n",
        "annotation_reason = weave.publish(AnnotationSpec(\n",
        "    name=\"Reason\",\n",
        "    description=\"Reason why you chose this value, write before clicking.\",\n",
        "    field_schema={ \"type\": \"string\"},\n",
        "), \"reason\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "as9JYl0858Tv"
      },
      "outputs": [],
      "source": [
        "# Initialize our LLM client, we'll use either Gemini or OpenAI\n",
        "API_PROVIDER = 'OpenAI' # @param [\"Gemini\", \"OpenAI\", \"OpenRouter\"]\n",
        "if API_PROVIDER == 'Gemini':\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
        "        base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
        "    )\n",
        "    model = \"gemini-2.0-flash-exp\"\n",
        "elif API_PROVIDER == 'OpenRouter':\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "    )\n",
        "    model = \"openai/chatgpt-4o-latest\"\n",
        "    # model = \"google/gemini-flash-1.5-exp\"\n",
        "    # model = \"deepseek/deepseek-chat\"\n",
        "else:\n",
        "    client = OpenAI(api_key='')\n",
        "    model = \"gpt-4o\"\n",
        "\n",
        "# Load the Jinja2 environment\n",
        "env = Environment(loader=FileSystemLoader('templates'))\n",
        "template = env.get_template('post.html.jinja')\n",
        "\n",
        "# Load replies data\n",
        "def load_replies():\n",
        "    replies = []\n",
        "    # Load replies from both files\n",
        "    with open('data/replies_alpin.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "        replies.extend(data['thread']['replies'])\n",
        "    with open('data/replies_daniel.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "        replies.extend(data['thread']['replies'])\n",
        "    return replies\n",
        "\n",
        "\n",
        "def get_random_post_and_analyze():\n",
        "    replies = load_replies()\n",
        "    post = random.choice(replies)\n",
        "\n",
        "    # Format the post data for the template\n",
        "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
        "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
        "\n",
        "    # Convert AT URI to bsky.app URL\n",
        "    at_uri = post['post']['uri']\n",
        "    _, _, author_did, _, post_id = at_uri.split('/')\n",
        "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
        "\n",
        "    # Analyze the post\n",
        "    #download the avatar and convert to PIL image\n",
        "    avatar_uri = post['post']['author'].get('avatar')\n",
        "    avatar_response = requests.get(avatar_uri)\n",
        "    avatar_pil = Image.open(io.BytesIO(avatar_response.content))\n",
        "\n",
        "    response_dict = analyze_post_sentiment(avatar_pil, post['post']['author']['displayName'], post['post']['record']['text'])\n",
        "    analysis = response_dict['llm_classification']\n",
        "    weave_call_id = response_dict['weave_call_id']\n",
        "\n",
        "    post_data = {\n",
        "        'author': post['post']['author'],\n",
        "        'created_at': formatted_date,\n",
        "        'text': post['post']['record']['text'],\n",
        "        'like_count': post['post'].get('likeCount', 0),\n",
        "        'repost_count': post['post'].get('repostCount', 0),\n",
        "        'has_image': False,\n",
        "        'post_url': post_url\n",
        "    }\n",
        "\n",
        "    return template.render(**post_data), analysis, weave_call_id, ''\n",
        "\n",
        "\n",
        "def submit_feedback(user_selection, reason, weave_call_id):\n",
        "    \"\"\"\n",
        "    Example function that could send user feedback (the user_selection)\n",
        "    and the weave_call_id to your Weave (or any other) API.\n",
        "    \"\"\"\n",
        "    call = weave_api.get_call(weave_call_id)\n",
        "\n",
        "    if not call:\n",
        "        raise Exception('No Weave call ID found, have you tried adding @weave.op to the analyze_post_sentiment function?')\n",
        "\n",
        "    if reason:\n",
        "        reason_resp = weave_api.server.feedback_create(\n",
        "            {\n",
        "            \"project_id\": weave_api._project_id(),\n",
        "            \"weave_ref\": call.ref.uri(),\n",
        "            \"feedback_type\": \"wandb.annotation.reason\",\n",
        "            \"annotation_ref\": annotation_reason.uri(),\n",
        "            \"payload\": {\"value\": reason},\n",
        "            }\n",
        "        )\n",
        "\n",
        "    resp = weave_api.server.feedback_create(\n",
        "        {\n",
        "            \"project_id\": weave_api._project_id(),\n",
        "            \"weave_ref\": call.ref.uri(),\n",
        "            \"feedback_type\": \"wandb.annotation.doomer_or_boomer\",\n",
        "            \"annotation_ref\": annotation.uri(),\n",
        "            \"payload\": {\"value\": user_selection},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Ready to analyze the next post\n",
        "    return get_random_post_and_analyze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbzVTR3058Tv"
      },
      "source": [
        "# 1. Tracing LLM calls with Weave\n",
        "\n",
        "#### Why Tracing is Important for LLM Application Reliability\n",
        "\n",
        "In building reliable LLM-based applications, having a clear view into\n",
        "how your system behaves is crucial. That‚Äôs where ‚Äútracing‚Äù comes in.\n",
        "\n",
        "1. **Detailed Interaction Records**:\n",
        "   Tracing captures all the inputs, prompts, responses, and any user feedback.\n",
        "   By preserving this detailed record, you always have the context needed to\n",
        "   debug unexpected or incorrect results.\n",
        "\n",
        "2. **Rapid Issue Diagnosis**:\n",
        "   With thorough traces, you can pinpoint issues faster‚Äîoften without\n",
        "   needing direct access to remote systems. Simply reviewing the logs can\n",
        "   reveal how a certain response was triggered.\n",
        "\n",
        "3. **Collaboration and Sharing**:\n",
        "   Traces can be shared with both technical and non-technical stakeholders.\n",
        "   This not only streamlines collaboration but also ensures everyone is\n",
        "   working off the same ‚Äúsource of truth‚Äù when investigating bugs\n",
        "   or brainstorming improvements.\n",
        "\n",
        "4. **Outlier Spotting and Performance Tuning**:\n",
        "   By tracking calls at scale, you can detect when responses deviate\n",
        "   dramatically from the norm, troubleshoot any failures, and identify\n",
        "   potential performance bottlenecks.\n",
        "\n",
        "5. **Facilitates Product Evolution**:\n",
        "   As you enhance or expand your LLM application, comprehensive\n",
        "   tracing data helps you make more informed decisions about what to\n",
        "   improve, remove, or refine.\n",
        "\n",
        "With W&B Weave, comprehensive tracing is just 1 line of code, and offers features such as:\n",
        "- Syntax highlighting specific to your use-case (Markdown, JSON, etc.)\n",
        "- Ability to share links with other members of your team\n",
        "- Ability to filter traces by function name, input, output, etc.\n",
        "- Tracking latency, token count and cost per call (and trends)\n",
        "- Code associated with the llm call and versioning\n",
        "- Ability to add metadata per trace\n",
        "\n",
        "If you need to instrument existing code, you can use the `@weave.op` decorator to trace the function.  \n",
        "\n",
        "![CleanShot 2024-04-08 at 14 15 40@2x](https://gist.github.com/assets/463317/4e9ada49-572f-47d9-91e1-55ab72b2a476)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "1Cz22tA258Tv",
        "outputId": "fa8118b2-02e8-45d1-b31f-760b1e411256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/zkoticha/aie-llm-evals-workshop/r/call/01952ef9-64ab-7fa3-8d5f-93d40f8341ff\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2229c9ffff0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Lets test this out without tracing first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mresponse_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_post_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Alex\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"I hate AI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2229c9ffff0b>\u001b[0m in \u001b[0;36manalyze_post_sentiment\u001b[0;34m(avatar, displayName, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                     res, _ = _do_call(\n\u001b[0m\u001b[1;32m    670\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(op, __weave, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         execute_result = _execute_op(\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__should_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_execute_op\u001b[0;34m(__op, __call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mhandle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_execute_op\u001b[0;34m(__op, __call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mhandle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/integrations/openai/openai_sdk.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"include_usage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "#TODO 1: Add tracing to this function - then see how this function is traced in the Weave UI\n",
        "\n",
        "\n",
        "def analyze_post_sentiment(avatar, displayName, text):\n",
        "    # Prompt for OpenAI to analyze the sentiment\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither].\n",
        "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
        "    \\n\\n {displayName}: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO 2: Add some more context about our task to the prompt\n",
        "    # prompt = f\"\"\"Analyze the following Bluesky post and determine if the author is a:\n",
        "    # - DOOMER (someone who hates AI and uses derogatory language)\n",
        "    # - BOOMER (someone who doesn't understand AI and asks to remove their data)\n",
        "    # - NEITHER (neutral or positive response)\n",
        "\n",
        "    # Post: {displayName}: \"{text}\"\n",
        "\n",
        "    # Respond with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
        "    # \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        current_call = weave.require_current_call()\n",
        "        weave_call_id = current_call.id\n",
        "    except:\n",
        "        weave_call_id = None\n",
        "\n",
        "    return {\n",
        "        \"llm_classification\": response.choices[0].message.content,\n",
        "        \"weave_call_id\": weave_call_id\n",
        "    }\n",
        "\n",
        "# Lets test this out without tracing first\n",
        "response_dict = analyze_post_sentiment(\"\",\"Alex\",\"I hate AI\")\n",
        "\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZMvaO4u58Tv"
      },
      "source": [
        "We can see that even without @weave.op, since Weave is initialized, it will still trace the function call and store it in the Weave project as it automatically understands that we use OpenAi client. However if we add @weave.op, we can get even more detail and insrument our existing code with Weave.\n",
        "\n",
        "Tracing becomes even more useful when you have a lot of nested calls, such as a multi-step chat conversation, or a RAG system with retrieval, or an agentic system with multiple steps.\n",
        "\n",
        "![text](https://cln.sh/Sc8ZtrdM+)\n",
        "\n",
        "[Here's a great example](https://wandb.ai/wandb-designers/winston/weave/traces?cols=%7B%22attributes.weave.client_version%22%3Afalse%2C%22attributes.weave.os_name%22%3Afalse%2C%22attributes.weave.os_release%22%3Afalse%2C%22attributes.weave.os_version%22%3Afalse%2C%22attributes.weave.source%22%3Afalse%2C%22attributes.weave.sys_version%22%3Afalse%7D&peekPath=%2Fwandb-designers%2Fwinston%2Fcalls%2F0193ff3f-54d7-73a3-8004-0a582a594307%3Fpath%3Dwinston-solve*0%2Bvincent-execute*0%26tracetree%3D1) of a more complex traced setup from our internal agent system called Winston - with multiple tools selection, retrieval steps etc Winston Weave Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6pQ0c3P58Tv"
      },
      "source": [
        "\n",
        "# 2. User Feedback & Annotations\n",
        "\n",
        "Collecting user feedback is a crucial way to improve your LLM applications. There's a reason that every chatbot you use has üëç/üëé and a text box to leave feedback. This is one of the best ways for those labs to understand and improve their models and align them to user preferences.\n",
        "\n",
        "![text](https://cln.sh/JGMBxMtH+)\n",
        "\n",
        "Users don't have to be external as well, as you develop your application, marking traces as \"good\" or \"bad\", and adding why, is a great way to kick start your initial evaluation dataset with working and non-working examples.\n",
        "\n",
        "Additionally, after logging hundreds of thousads of traces, they will all start looking the same, so additional context like your user's feedback, will greately improve your ability to look at your data and find the outliers.\n",
        "\n",
        "Weave supports collecting user Feedback in the UI and also via the API so you can collect it from your users and also leave it yourself while looking at your data.\n",
        "\n",
        "![text](https://cln.sh/X6fFHD8t+)\n",
        "\n",
        "Read more about feedback [here](https://weave-docs.wandb.ai/guides/tracking/feedback)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmthLmwR58Tw"
      },
      "source": [
        "\n",
        "\n",
        "# 2.1 Doomer or Boomer App - Annotations by example\n",
        "\n",
        "Unlike user feedback, Annotations are a bit of a more structure way to classify responses, to help create a dataset of golden answers and reasons or rationales for those answers. All of the major companies use Scale.ai for this and pay them a LOT of money, but you don't have to right away, you can start small, by yourself or with your team.\n",
        "\n",
        "Let's see how we can kickstart a simple dataset of annotations by a practical example.\n",
        "\n",
        "![image](https://gist.github.com/user-attachments/assets/a8537545-e070-4c8e-9988-2a8a905b9d2c)\n",
        "\n",
        "To simulate a real world scenario, we'll build a simple app that will allow you to annotate a few posts.\n",
        "\n",
        "In our case, we're pretending to work at a company that's trying to build an AI classifier for Bluesky posts. We're humans that work in the company and are helping it to align and finetune models for AI moderation.\n",
        "\n",
        "We've compiled replies from BlueSky users, on 2 posts that collected publicly available data from BlieSky to train AI models (BlueSky data is public), which led to a lot of hate by users on BlueSky.\n",
        "\n",
        "We're going to build a simple app that will use an LLM to classify the replies into 3 categories: `Doomer`, `Boomer`, or `Neither`.\n",
        "\n",
        "`Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
        "`Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
        "`Neither`: Folks who reply neutral or positive to the post.\n",
        "\n",
        "At first our LLMs will not have context to the task, so won't be able to reliably classify the replies, so a human is needed to annotate with additional context, you are that human.\n",
        "\n",
        "Launch the app and go through a few posts, annotate with a reason for your choice and the correct classification, we'll later use this data to align/finetune our LLM to classify the replies more accuretly and reliably."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwpn4nFa58Tw"
      },
      "outputs": [],
      "source": [
        "# %%blocks\n",
        "# TODO 3 - Launch the Gradio app and annotate 10-20 examples according to the rules\n",
        "os.environ['WEAVE_PRINT_CALL_LINK'] = 'false'\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Add a title and description\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ü¶ã Doomer or Boomer\n",
        "    Our AI analyzes bluesky replies and posts to determine if the author is a doomer or a boomer.\n",
        "    Source of data: Replies to a post by a BlueSky user that compiled a dataset of posts, which went viral and generated a lot of hate on BlueSky.\n",
        "    These are replies and comments on 2 posts that collected a dataset of posts of BlueSky users to train AI models (BlueSky data is public)\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            post_html = gr.HTML()\n",
        "            next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
        "            gr.Markdown(f\"\"\"\n",
        "            #### Instructions for labeler:\n",
        "            `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI\n",
        "            `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset\n",
        "            `Neither`: Folks who reply neutral or positive to the post.\n",
        "\n",
        "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
        "            \"\"\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            analysis_output = gr.Textbox(\n",
        "                label=\"Analysis Results\",\n",
        "                placeholder=\"Analysis will appear here...\",\n",
        "                lines=4\n",
        "            )\n",
        "            weave_call_id_state = gr.State()\n",
        "\n",
        "            # Replace dropdown with three buttons\n",
        "            reason_input = gr.Textbox(label=\"Add reason and click\",placeholder=\"Reason why you chose this value, write before clicking.\", lines=2)\n",
        "            with gr.Row():\n",
        "                doomer_btn = gr.Button(\"Doomer üò°\", variant=\"huggingface\")\n",
        "                boomer_btn = gr.Button(\"Boomer üëµ\", variant=\"primary\")\n",
        "                neither_btn = gr.Button(\"Neither ü§∑\")\n",
        "\n",
        "\n",
        "    # Set up event handler for combined next/analyze\n",
        "    next_post_btn.click(fn=get_random_post_and_analyze, outputs=[post_html, analysis_output, weave_call_id_state, reason_input])\n",
        "\n",
        "    doomer_btn.click(\n",
        "    fn=submit_feedback,\n",
        "    inputs=[gr.State(\"Doomer\"), reason_input, weave_call_id_state],\n",
        "    outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
        "    )\n",
        "    boomer_btn.click(\n",
        "        fn=submit_feedback,\n",
        "        inputs=[gr.State(\"Boomer\"), reason_input, weave_call_id_state],\n",
        "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
        "    )\n",
        "    neither_btn.click(\n",
        "        fn=submit_feedback,\n",
        "        inputs=[gr.State(\"Neither\"), reason_input, weave_call_id_state],\n",
        "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
        "    )\n",
        "\n",
        "\n",
        "    # Initialize with first post and analysis\n",
        "    post_html.value, analysis_output.value, weave_call_id_state.value, reason_input.value = get_random_post_and_analyze()\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd2XgMRc58Tw"
      },
      "source": [
        "## 2.1 Building a dataset from annotated calls\n",
        "\n",
        "Now that we've annotated at least 10-20 examples, we can build our first evaluation dataset!\n",
        "\n",
        "![text](https://cln.sh/dyBq4QXD+)\n",
        "\n",
        "Step 1: Filter calls in Weave UI by only those with annotations not empty\n",
        "\n",
        "Step 2: Use the Export -> Use Python button to get code to extract a list of filtered annotated calls\n",
        "\n",
        "Step 3: Convert the calls to a clean evaluation dataset (and optionally publish to Weave)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho9uhEV158Tw"
      },
      "outputs": [],
      "source": [
        "#TODO 4- Export annotated calls from Weave, clean up and publish to a dataset\n",
        "\n",
        "@weave.op\n",
        "def get_annotated_calls():\n",
        "   # Weave API call to get all calls filtered by annotations not empty (with reasons)\n",
        "   resp = weave_api.server.calls_query_stream({\n",
        "      \"project_id\": weave_api._project_id(),\n",
        "      \"filter\": {\"op_names\": [f\"weave:///{weave_api._project_id()}/op/analyze_post_sentiment:*\"]},\n",
        "      \"query\": {\"$expr\":{\"$and\":[{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.doomer_or_boomer].payload.value\"},{\"$literal\":\"\"}]}]},{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.reason].payload.value\"},{\"$literal\":\"\"}]}]}]}},\n",
        "      \"sort_by\": [{\"field\":\"started_at\",\"direction\":\"desc\"}],\n",
        "      \"include_feedback\": True,\n",
        "   })\n",
        "\n",
        "   # Iterate over the calls, clean up and publish as a dataset we can version and reference later.\n",
        "   list_of_calls = []\n",
        "   dataset = []\n",
        "   for call in resp:\n",
        "      try:\n",
        "         row = {}\n",
        "         call_dict = dict(call)\n",
        "         row[\"input\"] = call_dict.get('inputs').get('text')\n",
        "         row[\"displayName\"] = call_dict.get('inputs').get('displayName')\n",
        "         row[\"llm_classification\"] = call_dict.get('output').get('llm_classification')\n",
        "         list_of_feedback = call_dict.get('summary').get('weave').get('feedback')\n",
        "         for feedback in list_of_feedback:\n",
        "            if feedback.get(\"feedback_type\") == 'wandb.annotation.doomer_or_boomer':\n",
        "               row[\"human_annotation\"] = feedback.get('payload').get('value')\n",
        "            if feedback.get(\"feedback_type\") == 'wandb.annotation.reason':\n",
        "               row[\"reason\"] = feedback.get('payload').get('value')\n",
        "      except Exception as e:\n",
        "        continue\n",
        "\n",
        "      dataset.append(row)\n",
        "\n",
        "   weave_dataset = weave.Dataset(name=\"doomer_or_boomer_dataset\", rows=dataset)\n",
        "   # TODO: Uncomment this to publish the dataset\n",
        "   # weave.publish(doomer_or_boomer_dataset)\n",
        "   return weave_dataset\n",
        "\n",
        "doomer_or_boomer_dataset = get_annotated_calls()\n",
        "df = pd.DataFrame(doomer_or_boomer_dataset.rows)\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4RiKrra58Tw"
      },
      "source": [
        "## 2.2 Storing Datasets within Weave\n",
        "\n",
        "If you'd like to store your own dataset and name them, it's very easy to do so, and then you get a \"ref\" to the dataset that's stored in our system. Weave datasets are versioned, which means you can reference them in your code by a URL or a ref, and either point to the latest version or a specific version.\n",
        "\n",
        "Using `refs` is a great way to make your code reproducible and versioned.\n",
        "\n",
        "![CleanShot 2025-01-07 at 16 12 35@2x](https://gist.github.com/user-attachments/assets/e2d02340-cc0f-41e8-8d97-957b08611d08)\n",
        "\n",
        "\n",
        "Here's an example of the dataset we just created, and how we can reuse it in our evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vql-Qi_m58Tw"
      },
      "outputs": [],
      "source": [
        "# TODO 5: replace this dataset with your own ref using the dataset link above and looking at the \"use\" tab\n",
        "doomer_or_boomer_dataset = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset:iCO7tzGYA3ow5dgj0gRb8J5p0fRYYpAwsK6TI6LOsSo\").get()\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doomer_or_boomer_dataset.rows)\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQyB--I758Tw"
      },
      "source": [
        "# Step 3 : Evaluations\n",
        "### Components of an Evaluation\n",
        "\n",
        "Evaluations generally consist of four key elements:\n",
        "- An **input prompt** that serves as the basis for the model's completion. This prompt often includes a set of variable inputs that are inserted into a prompt template during testing.\n",
        "- The **output** generated by the model in response to the input prompt.\n",
        "- A **\"gold standard\" answer** used as a reference for assessing the model's output. This can be an exact match that the output must replicate, or an exemplary answer that provides a benchmark for scoring.\n",
        "- A **score**, determined by one of the scoring approaches outlined below, which indicates the model's performance on the question.\n",
        "\n",
        "#TODO 6: Look at the dataset and try to match the input, output, gold standard each row\n",
        "\n",
        "## Evaluation Grading Approaches\n",
        "Evaluations can be time-consuming and costly in two main areas: creating questions and gold standard answers, and the scoring/grading process itself.  \n",
        "Developing questions and ideal answers is often a one-time fixed cost, albeit potentially time-intensive if a suitable dataset is not readily available (consider leveraging an LLM to generate questions!). However, scoring is a recurring expense incurred each time the evaluation is conducted, which is likely to be frequent. Therefore, designing evaluations that can be scored efficiently and economically should be a central priority.\n",
        "\n",
        "![](https://gist.github.com/assets/463317/e970bb03-9552-4712-ba12-727b89928e3b)\n",
        "\n",
        "There are three primary methods for grading (scoring) evaluations:  \n",
        "- **Programmatic:** This approach involves using standard code (primarily string matching and regular expressions) to assess the model's outputs. Common techniques include checking for an exact match against an answer or verifying the presence of key phrase(s) in a string. Programmatic scoring is the most optimal method when feasible, as it is extremely fast and highly reliable. However, not all evaluations are amenable to this style of scoring.\n",
        "  - Goes great with structured output - validate against an enum\n",
        "  - Code generation output - does it run, is valid, does it compile?\n",
        "  - Tool use validation - do the tools exist?  \n",
        "\n",
        "- **Human in the loop:** In this approach, a human reviewer examines the model-generated answer, compares it to the gold standard, and assigns a score. While manual scoring is the most versatile method, applicable to nearly any task, it is also exceptionally slow and costly, especially for large-scale evaluations. Designing evaluations that necessitate manual scoring should be avoided whenever possible.\n",
        "  - Domain specific & expert information\n",
        "  - Sensitive topics  \n",
        "\n",
        "- **Model-based scoring AKA LLM as a judge:** LLMs (especially Claude, GPT-4o, Gemini) are really good at grading themselves (or even outputs of other LLMs) especially in wide range of tasks that traditionally needed human judgement like tone in creative writing or accuracy in open-ended question, or classification. This model-based scoring is accomplished by creating a _scorer prompt_ for an LLM\n",
        "  - Open ended style questions\n",
        "  - Classification & Translation\n",
        "  - Instruction following\n",
        "\n",
        "Let's explore an example of each\n",
        "\n",
        "## 3.1 Programmatic scoring\n",
        "\n",
        "Here we have a simple programmatic eval that will try and check if the LLM had the right answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t0I-yG_58Tw"
      },
      "outputs": [],
      "source": [
        "## Create a programmatic scorer that will compare the ground truth to the LLM answer and check if it is correct\n",
        "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
        "import weave\n",
        "from weave import Evaluation\n",
        "\n",
        "def programmatic_scorer(output: str, human_annotation: str):\n",
        "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
        "    # we expect this evaluation to fail becuase the LLM is talking alot and never returns just the reason\n",
        "    if not output or not human_annotation:\n",
        "        raise ValueError(\"Model output or human annotation is empty\")\n",
        "    return {\"match\": output == human_annotation}\n",
        "\n",
        "# TODO 7: change the programmatic scorer (commented below) to check if the output includes the reason string (Doomer, Boomer, Neither)\n",
        "# check for lower case and upper case, and check if more than one of the options is present, meaning that LLM wasn't sure\n",
        "# add the programmatic scorer to the evaluation\n",
        "\n",
        "\n",
        "\n",
        "# def programmatic_scorer(output: str, human_annotation: str):\n",
        "#     # check if the first 4 letters of model output matches first 4 letters of human_annotation\n",
        "#     if not output or not human_annotation:\n",
        "#         raise ValueError(\"Model output or human annotation is empty\")\n",
        "\n",
        "#     # Convert both to lowercase and get first 4 letters\n",
        "#     output_start = output.lower()[:4]\n",
        "#     annotation_start = human_annotation.lower()[:4]\n",
        "\n",
        "#     return {\"match\": output_start == annotation_start}\n",
        "\n",
        "evaluation = Evaluation(\n",
        "    dataset=doomer_or_boomer_dataset, scorers=[programmatic_scorer]\n",
        ")\n",
        "\n",
        "@weave.op()\n",
        "def function_to_evaluate(input: str):\n",
        "    # here's where you would add your LLM call and return the output\n",
        "    # since we already called the LLM, we can just iterate over the dataset\n",
        "    # and return the llm_classification where the question is the same\n",
        "    row = [row for row in doomer_or_boomer_dataset.rows if row['input'] == input]\n",
        "    return row[0].get('llm_classification')\n",
        "\n",
        "await evaluation.evaluate(function_to_evaluate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy9F4Cv-58Tw"
      },
      "source": [
        "### 3.1.1 Structured outputs with programmatic scorers\n",
        "\n",
        "The above example likely gave us a score of 0, because LLMs like to talk, and comparing that via a simple string match is not going to work.\n",
        "\n",
        "Programmatic scorers work great when we have structured outputs and we know exactly what to expect from LLMs. Let's recreate our LLM calls for the same questions with strucutred outputs so we can compare the LLM output directly to the human annotation and see if we can get a better score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3SG-ycq58Tw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['WEAVE_PARALLELISM'] = '5'\n",
        "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
        "\n",
        "@weave.op()\n",
        "def with_structured_llm_call(input: str, displayName: str):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following Bluesky post and determine if the author is a [Doomer, Boomer, or Neither].\n",
        "    Be concise and to the point. Answer with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
        "\n",
        "\n",
        "    Text to Classify:\n",
        "    \\n\\n {displayName}: \"{input}\"\n",
        "    \"\"\"\n",
        "\n",
        "    ## TODO 8: add a request for structured output in JSON format\n",
        "    # prompt += \"\"\"\n",
        "    # Respond in JSON format with this exact schema   {{\n",
        "    #     \"classification\": \"DOOMER\" | \"BOOMER\" | \"NEITHER\",\n",
        "    #     \"reason\": \"string\"\n",
        "    # }}\n",
        "\n",
        "    # \"\"\"\n",
        "\n",
        "    ## TODO 9: request a stricter JSON\n",
        "    # prompt += \"\"\"\n",
        "    #     with no backticks or quotes or anything else, just valid JSON or I lose my job\n",
        "    # \"\"\"\n",
        "\n",
        "    ## TODO 10: Add additional context about the classification criteria (by copying the definition from above cells)\n",
        "    #  - but first try them in Weave playground\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "\n",
        "            {\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def programmatic_scorer(output: str, human_annotation: str):\n",
        "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
        "    if not output:\n",
        "        raise ValueError(\"Model output is empty\")\n",
        "    try:\n",
        "        object = json.loads(output)\n",
        "    except:\n",
        "        raise ValueError(\"Model output is not valid JSON\")\n",
        "\n",
        "    return {\"match\": object.get('classification').lower() == human_annotation.lower()}\n",
        "\n",
        "new_evaluation = Evaluation(\n",
        "    dataset=doomer_or_boomer_dataset, scorers=[programmatic_scorer]\n",
        ")\n",
        "\n",
        "await new_evaluation.evaluate(with_structured_llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CkC2fCZ58Tx"
      },
      "source": [
        "# 3.2 HITL - Human in the loop evaluation grading\n",
        "\n",
        "Programmatic scoring is great for many reasons, cheap to get started with, can run very fast and can be very reliable, but cannot cover open ended questions or tasks that require analysis or judgement.\n",
        "\n",
        "For example, did the LLM follow the instructions it was given, did it hallucinate, was it verbose or concise, etc.\n",
        "\n",
        "To judge those outputs we can use human graders, to provide \"golden answers\", which is what we did above with the annotation example with our Doomer or Boomer app.\n",
        "\n",
        "The downside of HITL is that it's slow, expensive, and not scalable (unless you have a lot of money in the bank).\n",
        "\n",
        "HITL is a great way to kickstart an evaluation dataset and extarpolate with an LLM.\n",
        "\n",
        "Here's a slight alternative on our app, that shows LLM responses and allows our humans in the loop to judge the responses as correct or incorrect.\n",
        "\n",
        "#TODO 11 - Run this app, mark up to 10 responses, and then hit \"run evaluations\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDDNpYDS58Tx"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "from weave import Evaluation\n",
        "dataset_of_doomer_or_boomer = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset_with_structured_output:EwbD2kvMzz1R8nY6IxY6EALPj7XV6XYue44gQWDgDKE\").get()\n",
        "\n",
        "def match_dataset_with_replies():\n",
        "    matched_replies = []\n",
        "    for row in dataset_of_doomer_or_boomer.rows:\n",
        "        # Find matching reply in all_replies\n",
        "        for reply in load_replies():\n",
        "            if reply['post']['record']['text'] == row['input']:\n",
        "                matched_reply = {\n",
        "                    'full_reply': reply,\n",
        "                    'input': row.get('input', ''),\n",
        "                    'output': row.get('output', ''),\n",
        "                    'reason': row.get('reason', ''),\n",
        "                    'llm_classification': row.get('llm_classification', ''),\n",
        "                    'displayName': row.get('displayName', '')\n",
        "                }\n",
        "                matched_replies.append(matched_reply)\n",
        "                break\n",
        "    return matched_replies\n",
        "\n",
        "matched_replies = match_dataset_with_replies()\n",
        "annotated_rows = []\n",
        "\n",
        "def get_next_annotated_post(current_index:int = 0):\n",
        "    # Get the matched replies\n",
        "\n",
        "    print(current_index, len(matched_replies))\n",
        "    if current_index >= len(matched_replies):\n",
        "        current_index = 0  # Reset to beginning if we've reached the end\n",
        "\n",
        "    reply = matched_replies[current_index]\n",
        "    post = reply['full_reply']\n",
        "\n",
        "    # Format the post data for the template\n",
        "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
        "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
        "\n",
        "    # Convert AT URI to bsky.app URL\n",
        "    at_uri = post['post']['uri']\n",
        "    _, _, author_did, _, post_id = at_uri.split('/')\n",
        "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
        "\n",
        "    post_data = {\n",
        "        'author': post['post']['author'],\n",
        "        'created_at': formatted_date,\n",
        "        'text': post['post']['record']['text'],\n",
        "        'like_count': post['post'].get('likeCount', 0),\n",
        "        'repost_count': post['post'].get('repostCount', 0),\n",
        "        'has_image': False,\n",
        "        'post_url': post_url\n",
        "    }\n",
        "\n",
        "    # Use the stored LLM classification and human annotation\n",
        "    analysis = f\"\"\"LLM Classification: {reply['llm_classification']}\n",
        "\n",
        "LLM Reasoning: {reply['reason']}\n",
        "    \"\"\"\n",
        "\n",
        "    run_evaluation_btn = {\n",
        "        \"interactive\": True if len(annotated_rows) >=  10 else False,\n",
        "        \"value\": \"Run Evaluation\" if len(annotated_rows) >=  10 else f\"Annotate {10 - len(annotated_rows)} more posts\"\n",
        "    }\n",
        "    return template.render(**post_data), analysis, current_index + 1, gr.update(**run_evaluation_btn), \"\"\n",
        "\n",
        "def submit_hitl_feedback(correct_or_incorrect: str, feedback: str, next_index: int):\n",
        "    annotated_rows.append({\n",
        "        \"input\": matched_replies[next_index-1].get('input'),\n",
        "        \"output\": matched_replies[next_index-1].get('output'),\n",
        "        \"llm_classification\": matched_replies[next_index-1].get('llm_classification'),\n",
        "        \"correct_or_incorrect\": True if correct_or_incorrect == \"correct\" else False,\n",
        "        \"human_reason_for_correct_or_incorrect\": feedback,\n",
        "    })\n",
        "    return get_next_annotated_post(next_index)\n",
        "\n",
        "\n",
        "def right_according_to_human(output: str, correct_or_incorrect: bool):\n",
        "    return correct_or_incorrect\n",
        "\n",
        "@weave.op()\n",
        "def return_input_row(input: str):\n",
        "    return [x for x in annotated_rows if x.get('input') == input]\n",
        "\n",
        "async def run_evaluation():\n",
        "    hitl_evaluation = Evaluation(\n",
        "        dataset=annotated_rows,\n",
        "        scorers=[right_according_to_human],\n",
        "        name=\"hitl_evaluation\"\n",
        "    )\n",
        "\n",
        "    result = await hitl_evaluation.evaluate(return_input_row)\n",
        "    gr.Info('Evaluation complete! Check your Weave project for the results.')\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvvsocL-58Tx"
      },
      "outputs": [],
      "source": [
        "# %%blocks\n",
        "# Create a Gradio Blocks app\n",
        "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as new_demo:\n",
        "    # Add a title and description\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Human in the loop\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(f\"\"\"## 1. Post to Analyze  \"\"\")\n",
        "            post_html = gr.HTML()\n",
        "            # next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
        "            gr.Markdown(f\"\"\"\n",
        "            #### Instructions for HTIL judge:\n",
        "            - review LLM outputs and mark them as correct or incorrect\n",
        "            - after 10-20 examples, hit \"run evaluation\" button\n",
        "\n",
        "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "\n",
        "            analysis_output = gr.Textbox(\n",
        "                label=\"2. Review LLM Classification for this post\",\n",
        "                placeholder=\"Analysis will appear here...\",\n",
        "                lines=4,\n",
        "            )\n",
        "            next_index = gr.State(value=0)\n",
        "\n",
        "            with gr.Accordion(\"Reminder of Doomer, Boomer, or Neither Criteria\", open=False):\n",
        "                gr.Markdown(f\"\"\"\n",
        "                `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI\n",
        "                `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset\n",
        "                `Neither`: Folks who reply neutral or positive to the post.\n",
        "                \"\"\")\n",
        "            # Replace dropdown with three buttons\n",
        "            reason_input = gr.Textbox(label=\"3. Add reason and submit\",placeholder=\"Reason why the LLM got this classification right or wrong\", lines=2)\n",
        "            with gr.Row():\n",
        "                correct_btn = gr.Button(\"LLM is Correct üëç\")\n",
        "                incorrect_btn = gr.Button(\"LLM is Incorrect üëé\")\n",
        "\n",
        "            run_evaluation_btn = gr.Button(\"Run Evaluation\", variant=\"primary\", interactive=False)\n",
        "\n",
        "\n",
        "    # Set up event handler for combined next/analyze\n",
        "    # next_post_btn.click(fn=get_next_annotated_post, inputs=[next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
        "\n",
        "    correct_btn.click(fn=submit_hitl_feedback, inputs=[gr.State(\"correct\"), reason_input, next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
        "    incorrect_btn.click(fn=submit_hitl_feedback, inputs=[gr.State(\"incorrect\"), reason_input, next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
        "\n",
        "    run_evaluation_btn.click(fn=run_evaluation, inputs=[], outputs=[analysis_output])\n",
        "    # Initialize with first post and analysis\n",
        "    post_html.value, analysis_output.value, next_index.value, run_evaluation_btn.value, reason_input.value = get_next_annotated_post()\n",
        "\n",
        "new_demo.queue()\n",
        "new_demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECH5blmH58Tx"
      },
      "source": [
        "# 3.3 LLM as a Judge - use another LLM to grade your LLM outputs\n",
        "\n",
        "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way!\n",
        "\n",
        "We can actually have an LLM do the grading for us. We'll use a teacher model to grade the LLM outputs of a \"student\" model (in this case the LLM we're using for our production system is the student).\n",
        "\n",
        "There are a few issues with this approaches to be aware of:\n",
        " - LLMs are not great at numerical scoring (eg 1-5)\n",
        " - The order of canditate responses matter\n",
        " - Foundational models tend to prefer their own outputs over other models\n",
        " - LLMs prefer longer respones and \"style\" over accuracy\n",
        "\n",
        "\n",
        "## 3.3.1 Let's build our LLM judge\n",
        "\n",
        "First, we'll start by building a \"grader prompt\" template, a prompt asking our judge to perform the judging itself. This will be our iteration grounds. In this template, we'll inject both the output of our production LLM model, and the criteria / rules or rubric that makes an answer correct or incorrect.\n",
        "\n",
        "In our case, the classification into one of 3 (Doomer, Boomer, Neither) is done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5X_Psil58Tx"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Build a grader prompt\n",
        "import weave\n",
        "from weave import Evaluation\n",
        "import json\n",
        "\n",
        "def build_grader_prompt(input: str, llm_classification: str, displayName: str):\n",
        "    grader_prompt_template = f\"\"\"\n",
        "    You are provided with the following:\n",
        "    <input> is a comment made on social media and the handle of the person making the comment\n",
        "    <output> is a classification and reasoning that an automated assistant made about the comment\n",
        "    <criteria> is a set of guidelines and additional context for you to understand the input and the correct way to classify it\n",
        "\n",
        "    <input>\n",
        "    @{displayName}: {input}\n",
        "    </input>\n",
        "\n",
        "\n",
        "    <output>\n",
        "    {llm_classification}\n",
        "    </output>\n",
        "\n",
        "    <criteria>\n",
        "    For context, the responses you are classifying are to 2 announcements, made by AI enthusiasts who collected posts from the open protocol of bluesky\n",
        "    and posted about it on bluesky. They received a torrent of hateful commentary about that effort, including lawfare that's not based in any legal basis.\n",
        "    The folks who use derogatory language we consider Doomers, folks who just copy paste are likely just boomers.\n",
        "\n",
        "    Instructions of how to classify responders:\n",
        "    `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of their hate for AI and their data being used for AI.\n",
        "    `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset\n",
        "    `Neither`: Folks who reply neutral or positive to the post.\n",
        "    </criteria>\n",
        "\n",
        "    Your task is to understand from <output> which of the 3 choices did the automated assistant make and if its reasoning is valid.\n",
        "    First think through whether the the output is correct or incorrect based on the criteria and add your thinking,\n",
        "    then output your answer in JSON format with this exact schema (no backticks or quotes or anything else, just valid JSON):\n",
        "\n",
        "    {{\n",
        "        \"automated_assistant_classification\": \"doomer\" | \"boomer\" | \"neither\",\n",
        "        \"actual_classification\": \"doomer\" | \"boomer\" | \"neither\",\n",
        "        \"thinking\": \"string\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    return grader_prompt_template\n",
        "\n",
        "# Step 2 - Get our datasets\n",
        "#TODO 12 - replace this if you want with your annotated examples - make sure the stucture matches\n",
        "dataset_without_context = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset:kPkJew7ifAQDTiskCKUeYZPAjSagSILxHY0Ze9a72i8\").get()\n",
        "\n",
        "\n",
        "# Step 3 - Build our LLM Judge API function\n",
        "\n",
        "@weave.op()\n",
        "def llm_judge_api(input: str, llm_classification: str, displayName: str):\n",
        "    grader_prompt = build_grader_prompt(input, llm_classification, displayName)\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "\n",
        "            {\"role\": \"user\", \"content\": grader_prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    response = response.choices[0].message.content\n",
        "    print(response)\n",
        "\n",
        "    return json.loads(response)\n",
        "\n",
        "# Step 4 - Create a scorer\n",
        "\n",
        "def right_according_to_llm_judge(output: dict):\n",
        "    return {\"match\": output.get('automated_assistant_classification').lower() == output.get('actual_classification').lower()}\n",
        "\n",
        "# Step 5 - Run our evaluation\n",
        "\n",
        "no_context_evaluation = Evaluation(\n",
        "    dataset=dataset_without_context,\n",
        "    scorers=[right_according_to_llm_judge],\n",
        "    name=\"noContextEvaluation\"\n",
        ")\n",
        "\n",
        "await no_context_evaluation.evaluate(llm_judge_api, __weave={\"display_name\": \"No Context\"})\n",
        "\n",
        "#TODO 13: Create a dataset from the calls of LLM from programmattic evals (that include context and answers are better)\n",
        "#  and run the LLM as a judge on the second dataset and see improvement\n",
        "# or uncomment the below code and run it, then compare the two evaluations\n",
        "\n",
        "\n",
        "dataset_with_context = weave.ref(\"weave:///thursdai/jan-evals-workshop/object/doomer_or_boomer_dataset:iCO7tzGYA3ow5dgj0gRb8J5p0fRYYpAwsK6TI6LOsSo\").get()\n",
        "with_context_evaluation = Evaluation(\n",
        "    dataset=dataset_with_context,\n",
        "    scorers=[right_according_to_llm_judge],\n",
        "    name=\"withContextEvaluation\"\n",
        ")\n",
        "# await with_context_evaluation.evaluate(llm_judge_api, __weave={\"display_name\": \"With Context\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An3OuYk458Tx"
      },
      "source": [
        "# 3.3.2 Confirming the LLM judge is better  - Meta Evaluation\n",
        "\n",
        "![](https://cln.sh/JbyJ2qM2+)\n",
        "\n",
        "Just because we see a higher score right now doesn't mean actually that we did our job correctly. The higher score may come from our model being better, but also can come from the fact that our LLM judge is mistakenly grading!\n",
        "\n",
        "We may also want to play around with a model of the LLM judge itself (maybe a reasoning model) to see if we can improve the Judge, or tinker with its prompt some more, by providing better examples of what makes a correct or incorrect answer (from our HITL annotations from before!)\n",
        "\n",
        "The way to account for this is to actually do a meta-evaluation on the LLM judges itself with scores like Cohen's Kappa, which is a measure of inter-rater reliability.\n",
        "\n",
        "![](https://cln.sh/QDCzPFqD+)\n",
        "\n",
        "We can do so by comparing the LLM judge's scores to the human grader's scores, update the Judge itself and then run the evaluation again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtVZfcX158Tx"
      },
      "outputs": [],
      "source": [
        "from weave import Scorer\n",
        "from weave import Evaluation\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "class DoomerBoomerScorer(Scorer):\n",
        "    \"\"\"Custom scorer that calculates agreement between human and LLM annotations\n",
        "    for the Doomer/Boomer classification task. This scorer calculates Cohen's Kappa for the LLM judge vs the human grader\"\"\"\n",
        "\n",
        "    @weave.op()\n",
        "    async def score(self, output: dict, human_annotation: str):\n",
        "        \"\"\"Score each prediction by comparing human annotation to LLM classification.\n",
        "        Args:\n",
        "            output: The dict provided by the model being evaluated\n",
        "            human_annotation: The ground truth annotation (Doomer/Boomer/Neither)\n",
        "        Returns:\n",
        "            Dict with match and classifications for aggregation\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"match\": output.get('automated_assistant_classification').lower() == output.get('actual_classification').lower(),\n",
        "            \"llm_class\": output.get('automated_assistant_classification').lower(),\n",
        "            \"judge_class\": output.get('actual_classification').lower(),\n",
        "            \"human_class\": human_annotation.lower(),\n",
        "        }\n",
        "\n",
        "    @weave.op()\n",
        "    def summarize(self, score_rows: list) -> dict:\n",
        "        \"\"\"Calculate summary metrics including Cohen's Kappa.\n",
        "        Args:\n",
        "            score_rows: List of individual scoring results\n",
        "        Returns:\n",
        "            Dict with summary metrics\n",
        "        \"\"\"\n",
        "        # Extract valid classifications\n",
        "        valid_rows = [\n",
        "            row for row in score_rows\n",
        "            if row.get(\"human_class\") is not None and row.get(\"judge_class\") is not None\n",
        "        ]\n",
        "\n",
        "        if not valid_rows:\n",
        "            return {\n",
        "                \"metrics\": {\n",
        "                    \"accuracy\": 0.0,\n",
        "                    \"cohens_kappa\": 0.0,\n",
        "                    \"sample_size\": 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Calculate metrics\n",
        "        human_classes = [row[\"human_class\"].lower() for row in valid_rows]\n",
        "        llm_classes = [row[\"judge_class\"].lower() for row in valid_rows]\n",
        "\n",
        "        # Calculate Cohen's Kappa\n",
        "        kappa = cohen_kappa_score(human_classes, llm_classes)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        matches = [row.get(\"match\", False) for row in valid_rows]\n",
        "        accuracy = sum(matches) / len(matches) if matches else 0\n",
        "\n",
        "        return {\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"cohens_kappa\": kappa,\n",
        "                \"sample_size\": len(valid_rows)\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "smart_judge_evaluation = Evaluation(\n",
        "    dataset=dataset_with_context,\n",
        "    scorers=[right_according_to_llm_judge, DoomerBoomerScorer()],\n",
        "    name=\"withKappa\"\n",
        ")\n",
        "\n",
        "await smart_judge_evaluation.evaluate(llm_judge_api, __weave={\"display_name\": \"Smart Judge with Kappa\"})\n",
        "\n",
        "#TODO 15 - run the same evaluation with a \"dumb\" judge to see cappa difference\n",
        "\n",
        "@weave.op()\n",
        "def dumb_judge_api(input: str, llm_classification: str, displayName: str):\n",
        "    import re\n",
        "    grader_prompt = f\"\"\"\n",
        "    You are provided with the following:\n",
        "    <input> is a comment made on social media and the handle of the person making the comment\n",
        "    <output> is a classification and reasoning that an automated assistant made about the comment\n",
        "    <criteria> is a set of guidelines and additional context for you to understand the input and the correct way to classify it\n",
        "\n",
        "    <input>\n",
        "    @{displayName}: {input}\n",
        "    </input>\n",
        "\n",
        "\n",
        "    <output>\n",
        "    {llm_classification}\n",
        "    </output>\n",
        "\n",
        "    <criteria>\n",
        "    I don't have good criteria for you, try anyway\n",
        "    </criteria>\n",
        "\n",
        "    Your task is to understand from <output> which of the 3 choices did the automated assistant make and if its reasoning is valid.\n",
        "    First think through whether the the output is correct or incorrect based on the criteria and add your thinking,\n",
        "    then output your answer in JSON format with this exact schema (no backticks or quotes or anything else, just valid JSON):\n",
        "\n",
        "    {{\n",
        "        \"automated_assistant_classification\": \"doomer\" | \"boomer\" | \"neither\",\n",
        "        \"actual_classification\": \"doomer\" | \"boomer\" | \"neither\",\n",
        "        \"thinking\": \"string\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "\n",
        "            {\"role\": \"user\", \"content\": grader_prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    response = response.choices[0].message.content\n",
        "\n",
        "    return json.loads(response)\n",
        "\n",
        "\n",
        "dumb_judge_evaluation = Evaluation(\n",
        "    dataset=dataset_with_context,\n",
        "    scorers=[right_according_to_llm_judge, DoomerBoomerScorer()],\n",
        "    name=\"withKappa\"\n",
        ")\n",
        "await dumb_judge_evaluation.evaluate(dumb_judge_api, __weave={\"display_name\": \"Dumb Judge with Kappa\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXzsX4IF58Tx"
      },
      "source": [
        "# 3.3 Aligning our judges with human preferences - Meta evaluation\n",
        "\n",
        "This is a bit out of scope for our workshop, but for those who want to learn more, one we start running our LLM as a judge, we'll notice their shortcomings. They will be biased toward certain things, changing the order of the questions sometimes will yield different results etc'\n",
        "\n",
        "Also, the human graders understanding of the question will change during the annotation process itself.\n",
        "\n",
        "So a meta evaluation process is needed to understand how the judge itself is performing, and align the LLM judge with the additional inputs from HITL responses.\n",
        "\n",
        "Then we need to compare between the judges to empirically contrast and understand if we made a material difference.\n",
        "\n",
        "For more of a deep dive into this topic, W&B just published a course on evaluations, https://wandb.me/evals with more info\n",
        "\n",
        "# Recap and Additional resources\n",
        "\n",
        "You've made it all the way to the end of this notebook! By now you have got a hands on experience in implementing nearly all parts of the robust LLMs in production framework below:\n",
        "\n",
        "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
        "\n",
        "## Additional resources\n",
        "\n",
        "- Weave documentation - [weave docs](https://wandb.me/weave)\n",
        "- W&B Evaluations course - [evals course](https://wandb.me/evals)\n",
        "- Eugene Yan's excellent blog - [evaluating LLM evaluatiors](https://eugeneyan.com/writing/llm-evaluators/)\n",
        "- Who validates the validators - Shreya Shankar [Paper](https://arxiv.org/abs/2404.12272)\n",
        "- Hamel Housain - [your product needs evaluations](https://hamel.dev/blog/posts/evals/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}